---
title: "Project 3-Dogs, Fried Chicken or Blueberry Muffins?"
author: 'Group7: Vassily Carantino, Xin Gao, Lin Han, Yijia Li, Qian Shi'
date: "Nov 1, 2017"
output:
  html_document: default
  pdf_document: default
---


### Step 0: Prepare all needed packages
```{r, warning=FALSE}
# list.of.packages <- c("gbm","caret","randomForest","EBImage","xgboost","OpenImageR", "dplyr")
# new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
# if(length(new.packages))
#   {
#    install.packages(new.packages)
#    source("https://bioconductor.org/biocLite.R")
#    biocLite("EBImage")
#   }
library("gbm")
library("ggplot2")
library("caret")
library("randomForest")
library("EBImage")
library("xgboost")
library("OpenImageR")
library("dplyr")
```


### Step 1: Set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.feature = TRUE
run.cv = TRUE    
run.feature.train = TRUE    
run.feature.test = TRUE 
```


### Step 2: Construct new visual features

```{r feature}
# source("../lib/feature.R")
# set.seed(1)
# 
# img_dir<- "../data/training_set/images/"
# tm_feature<- c()
# if(run.feature)
# {
#   tm_feature <- system.time(rgb_feature <- feature(img_dir, export=TRUE))
# }
# cat("Time for constructing features is ", tm_feature[3], "s \n")
# 
# rgb_feature<- rgb_feature[ ,-1]
# label_train<- read.csv("../data/training_set/label_train.csv")[, 2]
# names(label_train)<- "y"

# rgb_feature$y<- label_train
# 
# training<- sample_frac(rgb_feature, 0.7, replace=FALSE)
# testing<- setdiff(rgb_feature, training, 'rows')
# 
# write.csv(training, file="../output/rgb_new_training.csv")
# write.csv(testing, file="../output/rgb_new_testing.csv")
```


### Step 3: Models Training and Parameters Selection

Training Gradient Boosting Model (baseline) and XGboost model on original SIFT features by cross validation.

```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
```

#### Baseline Model: GBM 

##### Parameters training on Original 5000 SIFT features

* Load SIFT features
```{r}
train_SIFT<- readRDS('../output/training.RData')
X_trainSIFT<- train_SIFT[, -5001]
y_trainSIFT<- train_SIFT$y
```

* Choice for parameter values
To find the best parameters for Gradient Boosting Model (GBM), we set interaction depth fixed to 3, trees numbers from 200 to 600, and shrinkage values from 0.01 to 0.07.
```{r}
trees_range<- seq(200, 600, 200)
shrinks_range<- seq(0.01, 0.07, 0.02)
```

* Find the best parameters by cross validation 
```{r}
# K<- 3
# if(run.cv){
#   best_paras<- train_gbm_para(X_trainSIFT, y_trainSIFT, shrinkage_range, trees_range, K)
# }
```

* The best parameters
```{r}
# if(run.cv){
#   best_shrink<- best_paras$shrink
#   best_trees<- best_paras$ntrees
# }
best_shrink<- 0.07
best_trees<- 600
```

* Visualize the outcomes of CV
```{r}
# if(run.cv){
#   acu_mat<- best_paras$acu_mat
#   
#   plot(shrinks_range, acu_mat[, 1], 
#        xlab='Shrinkage', ylab="Accuracy", main="Cross Validation Accuracy for GBM", 
#        type="b", pch=1, ylim=c(0.76, 0.84))
#   
#   for(i in 2:length(trees_range)){
#      lines(shrinks_range, acu_mat[, i], type='b', col=i, pch=i)
#   }
#   
#   legend("bottomright", paste("Trees = ", trees_range), 
#          col=1:length(trees_range), pch=1:length(trees_range), lty=1, cex=0.7)
# }
```
From the above plot we can see that, the best parameters for GBM model is shrinkage = 0.01 and n.trees = 600. However, this accuracy is greater than 80%. In order to avoid overfitting, we choose parameters corresponding to the second largest accuracy, that is shrinkage = 0.07 and n.trees = 600. 
Next, we use all kinds of features in this GBM model, like SIFT, HOG, HSV, RGB, using k folds cross-validation to get the average accuarcy. The resuls are showing below: 
           CV_Acu
    RGB1	 89.7%
    HSV    87.5%
    HOG	   82.2%
    SIFT	 81.0%
We can see that GBM with RGB1 features is the best combination.

##### Fit the best GBM on entire training SIFT and RGB features separately

* GBM & Original 5000 SIFT features
```{r}
train_SIFT<- readRDS('../output/training.RData')
X_trainSIFT<- train_SIFT[, -5001]
y_trainSIFT<- train_SIFT$y
tm_SIFT<- c()

if(run.feature.train){
  tm_SIFT<- system.time(train_SIFT_gbm<- gbm_train(X_trainSIFT, y_trainSIFT, best_shrink, best_trees))
}
cat("Train the best GBM on entire training SIFT features is ", tm_SIFT[3], "s \n")
#saveRDS(train_SIFT_gbm, file="../output/train_SIFT_gbm.RData")
```

* GBM & 1440 RGB features
```{r}
train_RGB<- read.csv('../output/rgb_training1.csv')
X_trainRGB<- train_RGB[, -1441]
y_trainRGB<- train_RGB$y
tm_RGB<- c()

if(run.feature.train){
  tm_RGB<- system.time(train_RGB_gbm<- gbm_train(X_trainRGB, y_trainRGB, best_shrink, best_trees))
}
cat("Train the best GBM on entire training RGB features is ", tm_RGB[3], "s \n")
#saveRDS(train_RGB_gbm, file="../output/train_RGB_gbm.RData")
```


#### Prefer Model: Xgboost

##### Parameters training on 1440 RGB features

* Load RGB features
```{r}
train_RGB<- read.csv('../output/rgb_feature1.csv')
X_trainRGB<- train_RGB[, -1441]
y_trainRGB<- train_RGB$y
```

* Find the best parameters by cross validation  
```{r}
# K<- 5
# if(run.cv){
#   xgbcv_result <- xgb_para(X_trainRGB, y_trainRGB, K)
# }
```

* The best parameters
```{r}
# if(run.cv){
#   best_par <- xgbcv_result[2]
#   par_best <- list(max_depth = best_par[[1]]$max_depth, eta = best_par[[1]]$eta,
#                    nrounds = 100, gamma = 0,
#                    nthread = 2, subsample = 0.5,
#                    objective = "multi:softprob", num_class = 3)
# }
par_best <- list(max_depth = 3, eta = 0.3,
                 nrounds = 100, gamma = 0,
                 nthread = 2, subsample = 0.5,
                 objective = "multi:softprob", num_class = 3)
```

* Visualize the outcomes of CV
```{r}
# if(run.cv){
#   error_mat <- 1-xgbcv_result[[1]]
#   plot(c(0.1,0.3,0.5), error_mat[1,], 
#        xlab='eta', ylab='CV Accuracy', main='Cross Validation Accuracy',
#        type='b', pch=1, ylim=c(0.85,0.95))
#   for(i in 2:3){
#     lines(c(0.1,0.3,0.5), error_mat[i,], type='b', col=i, pch=i)
#   }
#   legend('bottomright', paste('max_depth=', c(3,5,7)),
#          col=1:3, pch=1:3, lty=1, cex=0.6)
# }
```
For Xgboost Model, we tested parameters first, max_depth from (3,5,7), and eta from (0.1,0.3,0.5). 
From the above plot, we can get the best parameters are, max_depth = 3 and eta = 0.3, since it has the highest accuracy. 
Next, we used all kinds of features into this Xgboost model, like SIFT, HOG, HSV, RGB, using k folds cross-validation to get the accuarcy. The resuls are showing below: 
             CV_Acu
     RGB1	   90.7%
     HSV     89.3%
     HOG  	 84.8%
     SIFT	   79.8%
We can see that Xgboost Model with RGB1 features is the best combination.

##### Fit the best Xgboost model on entire training RGB features

```{r}
train_RGB<- read.csv('../output/rgb_training1.csv')
X_trainRGB<- train_RGB[, -1441]
y_trainRGB<- train_RGB$y

tm_train<- c()

if(run.feature.train){
  tm_train <- system.time(train_RGB_xgb <- xgb_train(X_trainRGB, y_trainRGB, par_best))
}

cat("Train the best Xgboost on entire training RGB features is ", tm_train[3], "s \n")
#saveRDS(train_RGB_xgb, file="../output/train_RGB_xgb.RData")
```


### Step 4: Make prediction on testing datasets

#### Fit the best GBM model on testing SIFT and RGB features separately

* GBM & Original 5000 SIFT features
```{r}
tm.SIFT_test_gbm<- c()
if(run.feature.test){
  test_SIFT<- readRDS('../output/testing.RData')
  X_test<- test_SIFT[, -5001]
  y_test<- test_SIFT$y
  
  #load(file="../output/train_SIFT_gbm.RData")
  
  tm.SIFT_test_gbm<- system.time(pred_SIFT_gbm<- gbm_test(train_SIFT_gbm, X_test))
  acu_SIFT_gbm<- sum(pred_SIFT_gbm == y_test) / length(y_test)
  
  cat("GBM accuracy for original SIFT features is ", acu_SIFT_gbm, "\n")
  cat("Time for fitting GBM on testing SIFT features is ", tm.SIFT_test_gbm[3], "s \n")
  
  #saveRDS(pred_SIFT_gbm, file="../output/pred_SIFT_gbm.RData")
}
```

* GBM & 1440 RGB features
```{r}
tm.RGB_test_gbm<- c()
if(run.feature.test){
   test_RGB<- read.csv('../output/rgb_testing1.csv')
   X_test<- test_RGB[, -1441]
   y_test<- test_RGB$y
   
   #load(file="../output/train_RGB_gbm.RData")
   
   tm.RGB_test_gbm<- system.time(pred_RGB_gbm<- gbm_test(train_RGB_gbm, X_test))
   acu_RGB_gbm<- sum(pred_RGB_gbm == y_test) / length(y_test)
   
   cat("GBM accuracy for RGB features is ", acu_RGB_gbm, "\n")
   cat("Time for fitting GBM on testing RGB features is ", tm.RGB_test_gbm[3], "s \n")
   
   #saveRDS(pred_RGB_gbm, file="../output/pred_RGB_gbm.RData")
}
```


#### Fit the best Xgboost model on testing RGB features

```{r}
tm_test<- c()
if(run.feature.test){
  test_RGB<- read.csv('../output/rgb_testing1.csv')
  X_test<- test_RGB[, -1441]
  y_test<- test_RGB$y
  
  #load(file="../output/train_RGB_xgb.RData")
  tm_test <- system.time(pred_RGB_xgb <- xgb_test(train_RGB_xgb, X_test))
  cat("Time for fitting Xgboost on testing RGB features is ", tm_test[3], "s \n")
  
  error <- sum(pred_RGB_xgb != y_test)/length(y_test)
  cat("Xgboost accuracy for RGB features is ", 1-error, "\n")
  
  #saveRDS(pred_RGB_xgb, file="../output/pred_RGB_xgb.RData")
}
```

